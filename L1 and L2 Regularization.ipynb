{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d75b22",
   "metadata": {},
   "source": [
    "### Name-Satyanistha Das\n",
    "### Regd.No-2241004100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86966a04",
   "metadata": {},
   "source": [
    "#  LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671fef1a",
   "metadata": {},
   "source": [
    "###### Lasso regression or L1 Regularization , also known as Least Absolute Shrinkage and Selection Operator (LASSO), is a statistical method used in machine learning and regression analysis. It's a powerful technique for addressing issues like overfitting and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c47a09",
   "metadata": {},
   "source": [
    "###### Key Points:\n",
    "\n",
    "###### Regularization: Lasso is a type of regularization technique. Regularization aims to balance the model's fit to the training data with its overall complexity. This helps prevent overfitting, where the model performs well on the training data but poorly on unseen data.\n",
    "\n",
    "###### L1 Penalty: Unlike regular linear regression, Lasso adds a penalty term to the cost function. This penalty term is based on the absolute value (L1 norm) of the coefficients. Coefficients with a smaller impact on the model are shrunk towards zero, and some may even be driven to exactly zero.\n",
    "\n",
    "###### Feature Selection: This shrinkage property of Lasso leads to feature selection. Features with minimal contribution to the model have their coefficients reduced to zero, effectively removing them from the model. This helps identify the most important features for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b1a17",
   "metadata": {},
   "source": [
    "###### Key advantages of Lasso regression:\n",
    "\n",
    "###### Prevents overfitting: By penalizing large coefficients, Lasso discourages the model from becoming overly complex and fitting too closely to random noise in the training data.\n",
    "\n",
    "###### Feature selection: Lasso performs automatic feature selection by driving unimportant features' coefficients to zero. This simplifies the model and improves interpretability.\n",
    "\n",
    "###### Better generalization: By avoiding overfitting and selecting relevant features, Lasso regression models often generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6deb21f",
   "metadata": {},
   "source": [
    "## Here is the gradient descent implementation of Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b43bf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 152  no. of iteration m:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03077056],\n",
       "       [0.59999328],\n",
       "       [0.15385282]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def predict(x,m):\n",
    "    prediction=np.dot(x,m)\n",
    "    return prediction\n",
    "def error(x,y,m):\n",
    "    return (y-predict(x,m))\n",
    "def cost(x,y,m):\n",
    "    e=0\n",
    "    ld=10000\n",
    "    for i in range(len(x)):\n",
    "        e=e+error(x[i],y[i],m)**2+ ld*abs(m[i])\n",
    "    return e/(2*len(x))\n",
    "def gradient(x,y,m):\n",
    "    temp=[]\n",
    "    for j in range(len(x[0])):\n",
    "        m_j=0\n",
    "        for i in range(len(x)):\n",
    "            m_j=m_j+(x[i][j]*error(x[i],y[i],m))\n",
    "        temp.append(m_j)\n",
    "    return temp\n",
    "def gradient_descent(x,y,m):\n",
    "    m=np.zeros((len(x[0]),1))\n",
    "    e=[]\n",
    "    iteration=[]\n",
    "    i=0\n",
    "    pre_cost=1\n",
    "    post_cost=0\n",
    "    while abs(post_cost-pre_cost)>0.001:\n",
    "        pre_cost=cost(x,y,m)\n",
    "        m=m+(np.array(gradient(x,y,m))*0.01)\n",
    "        post_cost=cost(x,y,m)\n",
    "        e.append(post_cost)\n",
    "        i+=1\n",
    "        iteration.append(i)\n",
    "    print('after',i,' no. of iteration','m:') \n",
    "    return m\n",
    "x=[[1,2,5],[1,7,5]]\n",
    "y=[2,5]\n",
    "m=[1,1,1]\n",
    "gradient_descent(x,y,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a72611",
   "metadata": {},
   "source": [
    "# RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57a91f",
   "metadata": {},
   "source": [
    "###### Ridge regression, also known as L2 regularization, is a technique used in linear regression to address overfitting and multicollinearity.\n",
    "\n",
    "######  Overfitting: This occurs when a model performs well on the training data but poorly on unseen data. Ridge regression helps by introducing a bias to the model, reducing its variance and complexity.\n",
    "######  Multicollinearity: This happens when independent variables in a regression model are highly correlated. Ridge regression helps by shrinking the coefficients of these correlated variables, reducing their influence on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66805edc",
   "metadata": {},
   "source": [
    "###### Key points:\n",
    "\n",
    "###### Ridge regression doesn't eliminate features entirely, unlike lasso regression (another regularization technique). It shrinks their coefficients.\n",
    "###### A hyperparameter called lambda controls the strength of the penalty term. A higher lambda leads to stronger shrinkage and potentially reduces variance but increases bias.\n",
    "###### Ridge regression is a good choice when dealing with multicollinearity or a high number of features relative to the number of observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17033e5a",
   "metadata": {},
   "source": [
    "###### There are actually two key advantages to ridge regression:\n",
    "\n",
    "###### Reduces Overfitting:  This is a major benefit. Standard linear regression can be prone to overfitting, where the model memorizes the training data too well and performs poorly on unseen data. Ridge regression introduces a bias that helps to  shrink the coefficients of the model, making it less complex and more generalizable to new data.\n",
    "\n",
    "###### Handles Multicollinearity:  This occurs when there's high correlation between independent variables in your data. In such cases, standard regression models can produce unreliable coefficient estimates with high variance. Ridge regression helps by shrinking the coefficients of these correlated variables, reducing their individual influence and leading to more stable and reliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39036c",
   "metadata": {},
   "source": [
    "## Here is the gradient descent implementation of Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f5416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 158  no. of iteration m is \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03077012],\n",
       "       [0.59999554],\n",
       "       [0.15385058]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def predict(x,m):\n",
    "    prediction=np.dot(x,m)\n",
    "    return prediction\n",
    "def error(x,y,m):\n",
    "    return (y-predict(x,m))\n",
    "def costf(x,y,m):\n",
    "    e=0\n",
    "    ld=10000\n",
    "    for i in range(len(x)):\n",
    "        e+=error(x[i],y[i],m)**2+ ld*m[i]**2\n",
    "    return e/(2*len(x))\n",
    "def gradient(x,y,m):\n",
    "    temp=[]\n",
    "    for j in range(len(x[0])):\n",
    "        m_j=0\n",
    "        for i in range(len(x)):\n",
    "            m_j=m_j+(x[i][j]*error(x[i],y[i],m))\n",
    "        temp.append(m_j)\n",
    "    return temp\n",
    "def gradient_descent(x,y,m):\n",
    "    m=np.zeros((len(x[0]),1))\n",
    "    e=[]\n",
    "    iteration=[]\n",
    "    i=0\n",
    "    pre_cost=1\n",
    "    post_cost=0\n",
    "    while abs(post_cost-pre_cost)>0.001:\n",
    "        pre_cost=costf(x,y,m)\n",
    "        m=m+(np.array(gradient(x,y,m))*0.01)\n",
    "        post_cost=costf(x,y,m)\n",
    "        e.append(post_cost)\n",
    "        i+=1\n",
    "        iteration.append(i)\n",
    "    print('after',i,' no. of iteration m is ') \n",
    "    return m\n",
    "x=[[1,2,5],[1,7,5]]\n",
    "y=[2,5]\n",
    "m=[1,1,1]\n",
    "gradient_descent(x,y,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89cf44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
